# Core Devs Community Call 25
### Meeting Date/Time: October 24th, 2024, 7:00-8:00 UTC
### Meeting Duration: 45 Mins
### [GitHub Agenda Page](https://github.com/tronprotocol/pm/issues/101)
### Agenda
* [ I cannot get results from api eth_getLogs sometimes](https://github.com/tronprotocol/java-tron/issues/6037)

### Details
* Jake

  Hello everyone, let’s stop waiting for others and start now. I will check and let them know once they show up. We only have one topic to discuss today. An issue was raised on GitHub, where people complained about the eth_getLogs. It seems that, for some reason under certain circumstances, this API does not perform normally and return what was requested.

  Brown, you have dug into this one. Would you mind sharing your findings with us?

* Brown

  Sure, one second.

  Everyone can see my screen, right? OK, this is a tricky one. Just now Jake briefly talked about it. I will explain the situation in detail.

  Someone in the community posted an issue regarding the return of the eth_getLogs interface. This interface requires an address and a specified block range to be passed in. Normally, it should return the logs and events related to the passed-in address. He gave an example in the issue, specifying an address and a range, but no log was returned. However, there are indeed logs for this address within this block range. He used the same interface with the JSON parameter and dropped the ‘address’ to verify it and successfully got the log. I tried to follow his steps to reproduce and found that the issue was real and it was a bug.

  I found other times in the block in his example and then used the relevant address of the relevant event as a parameter to call the eth_getLogs interface to query, and found that it could also be queried. But when using the address given in his issue to query, it could not be found. After that, I tried many other addresses and found that they could all query related events, which means the probability of this problem is extremely low. So I still focused on this specific address in his issue. I suspect that when this address is hashed in the code, the result may have a problem in the subsequent use of the code, that is to say, the problem may be caused by the specific result generated by the specific address, which leads to the occurrence of the exception.

  Based on this inference, I carefully look at the code of this interface and share it with you. The logic of writing and querying these logs to be queried is different, resulting in the writing conditions being more stringent than the querying conditions, which may be the cause of the problem. This interface has four main parameters, namely the address, two parameters of the block range, and the topic of the event. The data type of the address is an array, and the topic is a two-dimensional array. The length of the first dimension is 4, indicating the type of event, and the second dimension is unlimited, indicating the specific event.

  Now let's see how this event is written in. Actually, the main thing is to let the event pass through the bloom filter, and also use the bloom filter to search when looking for it. First, understand a definition. We define every 2048 blocks as a section. Each section corresponds to a key in the section bloom, and the value is a byte array with a length of 2048 * 2048, which is a matrix with an initial value of 0. When stored, it will be compressed with snappy and will be less than 4MB.

  After processing a block, traverse each transaction info in the transaction info list of this block. Each transaction info will have several events, and each event will have addresses and multiple topics. There are at most 4 topics in one event. The following is the data writing. First, generate the sha256 hash value of the address or topic; then apply 3 hash functions to the hash value to get 3 values, and the value range is [0, 2047]; then merge the positions of multiple events in a transaction, and then merge the positions of multiple transactions in a block to complete the writing of event data.

  Suppose the non-zero positions in this block are k0, k1 til km, the section corresponding to the block number is `s`, and the offset corresponding to the block number is f. Then read the bloom k0, bloom k1 til bloom km of section `s` from the database, set the f-th position to 1, and then write it back to the database. Only process the data of solidified blocks. Reading and writing only need one operation. This is the process of event writing.

  Now let's talk about data querying. When querying, a block range is given, and then the combination of address and topic is given, and then all events that meet the conditions are queried. First, calculate the sha256 hash of the address and the sha256 hash of each topic; then apply 3 hash functions to the hash value to get 3 values, and the value range is [0, 2047]. The following is the logical processing. Take the union of the values of the address and the topic. Suppose the positions of non-zero values are p0, p1 til pm; then split it into multiple sections according to the block number range: s, s + 1 til s + n. For section s, find the bitset values corresponding to bloom p0 to bloom pm, and then take the intersection of these m bitsets; suppose there are q non-zero values, that is, q offset blocks. Here, at most 10,000 block numbers are calculated. For the s-th section, the corresponding block number set is 2048 * s + the set of offset values. We take the transaction info list of the block corresponding to 2048 * s + the offset value, traverse the events corresponding to each transaction info, perform an exact match with the query conditions, and then merge and return all the results. If the block number range includes non-solidified blocks, directly read the in-memory data for traversal and matching. The method is the same. This is the whole process of event querying. This logic is actually much more complicated than what I said. Many conditions need to be considered, and it may not be possible to understand it just by looking at the code.

  Now let's talk about the cause of the problem mentioned in the issue. When the contract address is converted to bits by 3 hash functions and then written into the bitset, generally speaking, there will be 3 values after conversion, corresponding to 3 positions. However, there is a very small probability that there may be duplicate values among these three values, that is to say, two or even three of the values are the same, and ultimately there is a certain probability that the written value only occupies 2 or 1 position. However, in the reading logic, the bitset of the contract is forcibly set to 3 different values, and the unassigned ones default to 0, which is inconsistent with the data situation when writing, resulting in the inability to find it, because the length of the non-zero value found may be smaller than 3, which is 1 or 2. This is the cause of this problem. So here, adjust the code. Change the fixed-length data to ArrayList, and convert it to an array after writing the data, and this problem can be solved.

  Actually, it is a very simple problem. Essentially, it is a hash collision problem. Throwing 3 data into a hash bucket with a length of 2048 at the same time, the probability that at least 2 data occupy the same position is easily calculated. The denominator is all the situations, and the numerator is the number of collisions of two values.

  I downloaded the contract library and tried it. I scanned 2.5 million addresses and found that there are about 3,700 contract addresses that will have collision problems, with a probability of 0.148%, that is to say, given an address or topic, the probability of not being able to find it is about 0.148%. Here, everyone can simply understand it.

  Does anyone have any opinions or questions?

* Jake

  Regarding this problem, what measures should be taken currently?

* Brown

  It is here, `new int[3]`. Change the array here to an arraylist. Any amount of data can be put into it. In this way, the situation of having 0 value can be completely avoided, and the conditions for writing and querying can be guaranteed to be the same, so that this problem is solved.

* Jake

  Has this adjustment or measure been finalized?

* Brown

  Yes, I have tested it and there is no problem.

* Jake

  Currently, the 0.148% of addresses actually cannot be found under the current logic, because the value after the hash operation is certain, so there will definitely be a collision. Am I understanding correctly?

* Brown

  Yes, for those 3,700 addresses, no result can be returned under the current code operation. Unless it is a non-solidified block, that is, the event is traversed from the memory, because that does not need to pass through the bloom filter.

* Jake

  Then there should be no development difficulty. It is a very small adjustment. Is this adjustment expected to go online soon?

* Brown

  There is currently no clear plan. There is indeed no development difficulty. This problem is not very serious at present. The main thing is that the logic is rather complicated, and it is also very difficult to read the code. But after the problem is clarified, it is actually very simple. The go-live time has not been determined yet.

* Jake

  OK, thank you. Does anyone else have any questions?

  If there are no questions, today's Community Call is over. Thank you all for your time. Goodbye!


 

### Attendance
* Brown
* Andy
* Allen
* Daniel
* Super
* Murphy
* Jake